{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOijIYpgwvc1FXFHMSIKBZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSPDxu9WXD6j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8854bf54-84fd-4c7a-b409-20f829570870"
      },
      "source": [
        "!pip install faker"
      ],
      "execution_count": 426,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.6/dist-packages (4.1.2)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from faker) (1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.6/dist-packages (from faker) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.4->faker) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY1-zW_wWjAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 427,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3stGwhLcZxM6",
        "colab_type": "text"
      },
      "source": [
        "The module faker is used to create duplicate dates for our dataset\n",
        "\n",
        "The function format_date to convert those duplicate dates to human readable dates\n",
        "\n",
        "eg: 2007-03-01 to Mar 1,2007"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCPW516AXNcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fake=Faker()\n",
        "Faker.seed(1)\n",
        "random.seed(1)"
      ],
      "execution_count": 428,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRsaXAvEXkXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#kinds of human readable dates\n",
        "FORMATS = ['short',\n",
        "           'medium',\n",
        "           'long',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'd MMM YYY', \n",
        "           'd MMMM YYY',\n",
        "           'dd MMM YYY',\n",
        "           'd MMM, YYY',\n",
        "           'd MMMM, YYY',\n",
        "           'dd, MMM YYY',\n",
        "           'd MM YY',\n",
        "           'd MMMM YYY',\n",
        "           'MMMM d YYY',\n",
        "           'MMMM d, YYY',\n",
        "           'dd.MM.YY']"
      ],
      "execution_count": 429,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSBOzLKcXlee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_date():\n",
        "  \"\"\"\n",
        "  to create dates and return them as human readable\n",
        "  \"\"\"\n",
        "  #date_object returns a date betweeen January 1, 1970 and now\n",
        "  dt=fake.date_object()\n",
        "  try:\n",
        "    human_dates=format_date(dt,format=random.choice(FORMATS))\n",
        "    human_dates=human_dates.lower()\n",
        "    human_dates=human_dates.replace(\",\",\"\")\n",
        "    machine_dates=dt.isoformat()\n",
        "    #isoformat:'1999-11,13'\n",
        "  except Exception:\n",
        "    return None,None,None\n",
        "\n",
        "  return human_dates,machine_dates,dt\n"
      ],
      "execution_count": 430,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G5TkcX9Yydl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dateset(m):\n",
        "  \"\"\"\n",
        "  to load the dataset for the model\n",
        "  \"\"\"\n",
        "  human_vocab=set()\n",
        "  machine_vocab=set()\n",
        "  dataset=[]\n",
        "  for i in range(m):\n",
        "\n",
        "    #creating dataset and vocabolaries for machine and human readable dates\n",
        "    human_dates,machine_dates,_=load_date()\n",
        "    if human_dates!=None:\n",
        "      dataset.append((human_dates,machine_dates))\n",
        "      machine_vocab.update(list(machine_dates))\n",
        "      human_vocab.update(list(human_dates))\n",
        "  #checking print(human_vocab)\n",
        "  #assosiate each character with a item number in both vocaboaries \n",
        "  hum=dict(zip(sorted(human_vocab)+['<ukn>','<pad>'],list(range(len(human_vocab)+2))))\n",
        "  inverse_mac=dict(enumerate(sorted(machine_vocab)))\n",
        "  mac={ b:a for a,b in inverse_mac.items()}\n",
        "\n",
        "  return dataset,hum,mac,inverse_mac\n"
      ],
      "execution_count": 431,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHuAuKt6c4NQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "98d8b2d5-7574-4ff1-dd51-de17d6e5633e"
      },
      "source": [
        "#for checking\n",
        "\"\"\"\n",
        "for i in range(10):\n",
        "  a,b,c=load_date()\n",
        "  print(a,\"   \",b,\"   \",c)\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 432,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfor i in range(10):\\n  a,b,c=load_date()\\n  print(a,\"   \",b,\"   \",c)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 432
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Qk_B8sfz8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e0c1e41e-899b-499a-862b-3e63a2a1b51d"
      },
      "source": [
        "#for checking\n",
        "\"\"\"\n",
        "a,b,c,d=load_dateset(10)\n",
        "a\n",
        "\"\"\""
      ],
      "execution_count": 433,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\na,b,c,d=load_dateset(10)\\na\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 433
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLRucBShoFcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x,axis=1):\n",
        "  dim=K.ndim(x)\n",
        "  if dim==2:\n",
        "    K.softmax(x)\n",
        "  elif dim > 2:\n",
        "        #just for speed, the same formula\n",
        "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "        s = K.sum(e, axis=axis, keepdims=True)\n",
        "        return e / s\n",
        "  return K.softmax(x)"
      ],
      "execution_count": 434,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmxAmfyrsZLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_to_int(string,length,vocab):\n",
        "  \"\"\"\n",
        "  to convert the characters of a string to their respective indices as in vocablaries\n",
        "  \"\"\"\n",
        "  string=string.lower()\n",
        "  string=string.replace(\",\",\"\")\n",
        "  if len(string)>length:\n",
        "    string=string[:length]\n",
        "  replacement=list(map(lambda x: vocab.get(x,'<ukn>'),string))\n",
        "  if len(string)<length:\n",
        "    replacement=replacement + [vocab['<pad>']]*(length-len(string))\n",
        "  return replacement"
      ],
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJOq8SpGKpbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def int_to_string(ints,inv_vocab):\n",
        "  \"\"\"\n",
        "  inverse version of string to integer\n",
        "  \"\"\"\n",
        "  s=[inv_vocab[i] for i in ints]\n",
        "  return s"
      ],
      "execution_count": 436,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UucQH-WAUOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(dataset,hum,mac,Tx,Ty):\n",
        "  \"\"\"\n",
        "  preprocess the data to be used while training\n",
        "  \"\"\"\n",
        "  X,Y=zip(*dataset)\n",
        "  X=np.array([string_to_int(X[i],Tx,hum) for i in range(len(X))])\n",
        "  Y=[string_to_int(Y[i],Ty,mac) for i in range(len(Y))]\n",
        "  #onehot encodings \n",
        "  Xoh=np.array(list(map(lambda x:to_categorical(x,num_classes=len(hum)),X)))\n",
        "  Yoh=np.array(list(map(lambda y:to_categorical(y,num_classes=len(mac)),Y)))\n",
        "  return X,np.array(Y),Xoh,Yoh"
      ],
      "execution_count": 437,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOgwmm67Ji6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m=10000\n",
        "dataset,human_vocab,machine_vocab,inv_machine_vocab=load_dateset(m)"
      ],
      "execution_count": 438,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXqRg5TnKeQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Tx=30\n",
        "Ty=10\n",
        "X,Y,Xoh,Yoh=preprocess(dataset,human_vocab,machine_vocab,Tx,Ty)\n"
      ],
      "execution_count": 439,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnIuerykKdOV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a65d89f6-ca80-42e2-c10d-95140391c50b"
      },
      "source": [
        "Xoh.shape"
      ],
      "execution_count": 440,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 30, 37)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 440
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAMjOd0vSMoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model, Model"
      ],
      "execution_count": 441,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qIxOOSsQOJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#required layer objects needed to build the attention model\n",
        "\n",
        "#Attention unit\n",
        "\n",
        "repeator=RepeatVector(Tx)\n",
        "concatenator=Concatenate(axis=-1)\n",
        "densor1=Dense(15,activation='tanh')\n",
        "densor2=Dense(1,activation='relu')\n",
        "activator=Activation(softmax,name='acti_weights')\n",
        "dotor=Dot(axes=1)"
      ],
      "execution_count": 442,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SbB2954XNJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def step_attention(a,s_prev):\n",
        "  #repeat the 1-d s_prev Tx times\n",
        "  ss=repeator(s_prev)\n",
        "  #to concatenate activations of post activation lstm and pre-activation lstm\n",
        "  concat=concatenator([a,ss])\n",
        "  #intermediate layer for computing energies\n",
        "  inter_energy=densor1(concat)\n",
        "  #energies required to compute the alpha values\n",
        "  energy=densor2(inter_energy)\n",
        "  #activation softmax\n",
        "  alphas=activator(energy)\n",
        "  #dot product to calculate the context\n",
        "  context=dotor([alphas,a])\n",
        "  return context\n"
      ],
      "execution_count": 443,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6wF55aDh7o7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_a=16 #no of units of pre-activation lstm(bidirectional)\n",
        "n_s=32 #no of units of post activation lstm"
      ],
      "execution_count": 444,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyLj5UehlWJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "post_activation_lstm_cell=LSTM(n_s,return_state=True)\n",
        "out_layer=Dense(len(machine_vocab),activation=softmax)"
      ],
      "execution_count": 445,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpzc4ZgdjYcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(Tx,Ty,n_a,n_s,human_vocab,machine_vocab):\n",
        "  outputs=[]\n",
        "  #pre-attention \n",
        "  X_input=Input(shape=(Tx,len(human_vocab)))\n",
        "  a=Bidirectional(LSTM(n_a,return_sequences=True))(X_input)\n",
        "  \n",
        "  #post attention\n",
        "  s0=Input(shape=(n_s,))\n",
        "  c0=Input(shape=(n_s,))\n",
        "  s=s0\n",
        "  c=c0\n",
        "  for i in range(Ty):\n",
        "    context=step_attention(a,s)\n",
        "\n",
        "    #lstm layer for each timestep of output\n",
        "    s,_,c=post_activation_lstm_cell(context,initial_state=[s,c])\n",
        "    out=out_layer(s)\n",
        "    outputs.append(out)\n",
        "  \n",
        "  model=Model(inputs=[X_input,s0,c0],outputs=outputs)\n",
        "     \n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 446,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT3yrIU3osp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=model(Tx,Ty,n_a,n_s,human_vocab,machine_vocab)"
      ],
      "execution_count": 447,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFEc-5jdvxkB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5560a55-7674-4259-c714-ff544f8e0447"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 448,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_31\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_42 (InputLayer)           [(None, 30, 37)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_43 (InputLayer)           [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_17 (Bidirectional (None, 30, 32)       6912        input_42[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_8 (RepeatVector)  (None, 30, 32)       0           input_43[0][0]                   \n",
            "                                                                 lstm_26[0][0]                    \n",
            "                                                                 lstm_26[1][0]                    \n",
            "                                                                 lstm_26[2][0]                    \n",
            "                                                                 lstm_26[3][0]                    \n",
            "                                                                 lstm_26[4][0]                    \n",
            "                                                                 lstm_26[5][0]                    \n",
            "                                                                 lstm_26[6][0]                    \n",
            "                                                                 lstm_26[7][0]                    \n",
            "                                                                 lstm_26[8][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 30, 64)       0           bidirectional_17[0][0]           \n",
            "                                                                 repeat_vector_8[0][0]            \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 repeat_vector_8[1][0]            \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 repeat_vector_8[2][0]            \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 repeat_vector_8[3][0]            \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 repeat_vector_8[4][0]            \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 repeat_vector_8[5][0]            \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 repeat_vector_8[6][0]            \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 repeat_vector_8[7][0]            \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 repeat_vector_8[8][0]            \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 repeat_vector_8[9][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 30, 15)       975         concatenate_8[0][0]              \n",
            "                                                                 concatenate_8[1][0]              \n",
            "                                                                 concatenate_8[2][0]              \n",
            "                                                                 concatenate_8[3][0]              \n",
            "                                                                 concatenate_8[4][0]              \n",
            "                                                                 concatenate_8[5][0]              \n",
            "                                                                 concatenate_8[6][0]              \n",
            "                                                                 concatenate_8[7][0]              \n",
            "                                                                 concatenate_8[8][0]              \n",
            "                                                                 concatenate_8[9][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 30, 1)        16          dense_25[0][0]                   \n",
            "                                                                 dense_25[1][0]                   \n",
            "                                                                 dense_25[2][0]                   \n",
            "                                                                 dense_25[3][0]                   \n",
            "                                                                 dense_25[4][0]                   \n",
            "                                                                 dense_25[5][0]                   \n",
            "                                                                 dense_25[6][0]                   \n",
            "                                                                 dense_25[7][0]                   \n",
            "                                                                 dense_25[8][0]                   \n",
            "                                                                 dense_25[9][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "acti_weights (Activation)       (None, 30, 1)        0           dense_26[0][0]                   \n",
            "                                                                 dense_26[1][0]                   \n",
            "                                                                 dense_26[2][0]                   \n",
            "                                                                 dense_26[3][0]                   \n",
            "                                                                 dense_26[4][0]                   \n",
            "                                                                 dense_26[5][0]                   \n",
            "                                                                 dense_26[6][0]                   \n",
            "                                                                 dense_26[7][0]                   \n",
            "                                                                 dense_26[8][0]                   \n",
            "                                                                 dense_26[9][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_8 (Dot)                     (None, 1, 32)        0           acti_weights[0][0]               \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 acti_weights[1][0]               \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 acti_weights[2][0]               \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 acti_weights[3][0]               \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 acti_weights[4][0]               \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 acti_weights[5][0]               \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 acti_weights[6][0]               \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 acti_weights[7][0]               \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 acti_weights[8][0]               \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "                                                                 acti_weights[9][0]               \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "input_44 (InputLayer)           [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_26 (LSTM)                  [(None, 32), (None,  8320        dot_8[0][0]                      \n",
            "                                                                 input_43[0][0]                   \n",
            "                                                                 input_44[0][0]                   \n",
            "                                                                 dot_8[1][0]                      \n",
            "                                                                 lstm_26[0][0]                    \n",
            "                                                                 lstm_26[0][2]                    \n",
            "                                                                 dot_8[2][0]                      \n",
            "                                                                 lstm_26[1][0]                    \n",
            "                                                                 lstm_26[1][2]                    \n",
            "                                                                 dot_8[3][0]                      \n",
            "                                                                 lstm_26[2][0]                    \n",
            "                                                                 lstm_26[2][2]                    \n",
            "                                                                 dot_8[4][0]                      \n",
            "                                                                 lstm_26[3][0]                    \n",
            "                                                                 lstm_26[3][2]                    \n",
            "                                                                 dot_8[5][0]                      \n",
            "                                                                 lstm_26[4][0]                    \n",
            "                                                                 lstm_26[4][2]                    \n",
            "                                                                 dot_8[6][0]                      \n",
            "                                                                 lstm_26[5][0]                    \n",
            "                                                                 lstm_26[5][2]                    \n",
            "                                                                 dot_8[7][0]                      \n",
            "                                                                 lstm_26[6][0]                    \n",
            "                                                                 lstm_26[6][2]                    \n",
            "                                                                 dot_8[8][0]                      \n",
            "                                                                 lstm_26[7][0]                    \n",
            "                                                                 lstm_26[7][2]                    \n",
            "                                                                 dot_8[9][0]                      \n",
            "                                                                 lstm_26[8][0]                    \n",
            "                                                                 lstm_26[8][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 11)           363         lstm_26[0][0]                    \n",
            "                                                                 lstm_26[1][0]                    \n",
            "                                                                 lstm_26[2][0]                    \n",
            "                                                                 lstm_26[3][0]                    \n",
            "                                                                 lstm_26[4][0]                    \n",
            "                                                                 lstm_26[5][0]                    \n",
            "                                                                 lstm_26[6][0]                    \n",
            "                                                                 lstm_26[7][0]                    \n",
            "                                                                 lstm_26[8][0]                    \n",
            "                                                                 lstm_26[9][0]                    \n",
            "==================================================================================================\n",
            "Total params: 16,586\n",
            "Trainable params: 16,586\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qcd7dh7vyUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = Adam(lr=0.005)\n",
        "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 449,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr-nrQeOv1HU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s0=np.zeros((m,n_s))\n",
        "c0=np.zeros((m,n_s))\n",
        "outputs = list(Yoh.swapaxes(0,1))"
      ],
      "execution_count": 450,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2HrLMKSztRp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "741fdbdb-0a4d-49cf-8716-63f797e247f3"
      },
      "source": [
        "model.fit([Xoh,s0,c0],outputs,batch_size=32,epochs=15)"
      ],
      "execution_count": 451,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "313/313 [==============================] - 11s 36ms/step - loss: 12.6406 - dense_27_loss: 0.6808 - dense_27_1_loss: 0.5801 - dense_27_2_loss: 1.3778 - dense_27_3_loss: 2.4123 - dense_27_4_loss: 0.3222 - dense_27_5_loss: 0.7922 - dense_27_6_loss: 2.2926 - dense_27_7_loss: 0.3954 - dense_27_8_loss: 1.3903 - dense_27_9_loss: 2.3970 - dense_27_accuracy: 0.7202 - dense_27_1_accuracy: 0.8158 - dense_27_2_accuracy: 0.4274 - dense_27_3_accuracy: 0.1428 - dense_27_4_accuracy: 0.9706 - dense_27_5_accuracy: 0.6248 - dense_27_6_accuracy: 0.1818 - dense_27_7_accuracy: 0.9853 - dense_27_8_accuracy: 0.3649 - dense_27_9_accuracy: 0.1265\n",
            "Epoch 2/15\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 4.5427 - dense_27_loss: 0.0420 - dense_27_1_loss: 0.0311 - dense_27_2_loss: 0.3893 - dense_27_3_loss: 0.9666 - dense_27_4_loss: 0.0069 - dense_27_5_loss: 0.1486 - dense_27_6_loss: 1.1487 - dense_27_7_loss: 0.0104 - dense_27_8_loss: 0.7440 - dense_27_9_loss: 1.0552 - dense_27_accuracy: 0.9908 - dense_27_1_accuracy: 0.9935 - dense_27_2_accuracy: 0.8925 - dense_27_3_accuracy: 0.7307 - dense_27_4_accuracy: 0.9998 - dense_27_5_accuracy: 0.9563 - dense_27_6_accuracy: 0.6292 - dense_27_7_accuracy: 0.9996 - dense_27_8_accuracy: 0.7269 - dense_27_9_accuracy: 0.6766\n",
            "Epoch 3/15\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.3817 - dense_27_loss: 0.0074 - dense_27_1_loss: 0.0047 - dense_27_2_loss: 0.0199 - dense_27_3_loss: 0.0551 - dense_27_4_loss: 0.0020 - dense_27_5_loss: 0.0332 - dense_27_6_loss: 0.1070 - dense_27_7_loss: 0.0030 - dense_27_8_loss: 0.1012 - dense_27_9_loss: 0.0481 - dense_27_accuracy: 0.9996 - dense_27_1_accuracy: 0.9999 - dense_27_2_accuracy: 0.9993 - dense_27_3_accuracy: 0.9980 - dense_27_4_accuracy: 1.0000 - dense_27_5_accuracy: 0.9892 - dense_27_6_accuracy: 0.9831 - dense_27_7_accuracy: 1.0000 - dense_27_8_accuracy: 0.9761 - dense_27_9_accuracy: 0.9966\n",
            "Epoch 4/15\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.0789 - dense_27_loss: 0.0022 - dense_27_1_loss: 0.0017 - dense_27_2_loss: 0.0077 - dense_27_3_loss: 0.0260 - dense_27_4_loss: 7.7834e-04 - dense_27_5_loss: 0.0097 - dense_27_6_loss: 0.0124 - dense_27_7_loss: 0.0013 - dense_27_8_loss: 0.0091 - dense_27_9_loss: 0.0082 - dense_27_accuracy: 1.0000 - dense_27_1_accuracy: 1.0000 - dense_27_2_accuracy: 0.9996 - dense_27_3_accuracy: 0.9980 - dense_27_4_accuracy: 1.0000 - dense_27_5_accuracy: 0.9977 - dense_27_6_accuracy: 0.9998 - dense_27_7_accuracy: 1.0000 - dense_27_8_accuracy: 0.9988 - dense_27_9_accuracy: 1.0000\n",
            "Epoch 5/15\n",
            "313/313 [==============================] - 11s 36ms/step - loss: 0.0688 - dense_27_loss: 0.0017 - dense_27_1_loss: 0.0013 - dense_27_2_loss: 0.0063 - dense_27_3_loss: 0.0224 - dense_27_4_loss: 3.7749e-04 - dense_27_5_loss: 0.0063 - dense_27_6_loss: 0.0095 - dense_27_7_loss: 7.8532e-04 - dense_27_8_loss: 0.0122 - dense_27_9_loss: 0.0080 - dense_27_accuracy: 0.9999 - dense_27_1_accuracy: 1.0000 - dense_27_2_accuracy: 0.9994 - dense_27_3_accuracy: 0.9980 - dense_27_4_accuracy: 1.0000 - dense_27_5_accuracy: 0.9985 - dense_27_6_accuracy: 0.9999 - dense_27_7_accuracy: 1.0000 - dense_27_8_accuracy: 0.9975 - dense_27_9_accuracy: 0.9991\n",
            "Epoch 6/15\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.0341 - dense_27_loss: 6.7809e-04 - dense_27_1_loss: 6.0929e-04 - dense_27_2_loss: 0.0043 - dense_27_3_loss: 0.0190 - dense_27_4_loss: 2.7382e-04 - dense_27_5_loss: 0.0019 - dense_27_6_loss: 0.0029 - dense_27_7_loss: 3.6472e-04 - dense_27_8_loss: 0.0017 - dense_27_9_loss: 0.0024 - dense_27_accuracy: 1.0000 - dense_27_1_accuracy: 1.0000 - dense_27_2_accuracy: 0.9996 - dense_27_3_accuracy: 0.9980 - dense_27_4_accuracy: 1.0000 - dense_27_5_accuracy: 1.0000 - dense_27_6_accuracy: 1.0000 - dense_27_7_accuracy: 1.0000 - dense_27_8_accuracy: 1.0000 - dense_27_9_accuracy: 1.0000\n",
            "Epoch 7/15\n",
            "313/313 [==============================] - 11s 36ms/step - loss: 0.0308 - dense_27_loss: 4.8346e-04 - dense_27_1_loss: 4.5835e-04 - dense_27_2_loss: 0.0043 - dense_27_3_loss: 0.0181 - dense_27_4_loss: 1.9600e-04 - dense_27_5_loss: 0.0015 - dense_27_6_loss: 0.0021 - dense_27_7_loss: 2.8106e-04 - dense_27_8_loss: 0.0019 - dense_27_9_loss: 0.0016 - dense_27_accuracy: 1.0000 - dense_27_1_accuracy: 1.0000 - dense_27_2_accuracy: 0.9996 - dense_27_3_accuracy: 0.9979 - dense_27_4_accuracy: 1.0000 - dense_27_5_accuracy: 0.9999 - dense_27_6_accuracy: 1.0000 - dense_27_7_accuracy: 1.0000 - dense_27_8_accuracy: 0.9998 - dense_27_9_accuracy: 1.0000\n",
            "Epoch 8/15\n",
            "313/313 [==============================] - 11s 36ms/step - loss: 0.0264 - dense_27_loss: 3.2379e-04 - dense_27_1_loss: 3.6004e-04 - dense_27_2_loss: 0.0036 - dense_27_3_loss: 0.0172 - dense_27_4_loss: 1.6098e-04 - dense_27_5_loss: 0.0010 - dense_27_6_loss: 0.0015 - dense_27_7_loss: 1.9649e-04 - dense_27_8_loss: 9.2100e-04 - dense_27_9_loss: 0.0012 - dense_27_accuracy: 1.0000 - dense_27_1_accuracy: 1.0000 - dense_27_2_accuracy: 0.9996 - dense_27_3_accuracy: 0.9980 - dense_27_4_accuracy: 1.0000 - dense_27_5_accuracy: 0.9999 - dense_27_6_accuracy: 1.0000 - dense_27_7_accuracy: 1.0000 - dense_27_8_accuracy: 1.0000 - dense_27_9_accuracy: 1.0000\n",
            "Epoch 9/15\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.0229 - dense_27_loss: 2.4050e-04 - dense_27_1_loss: 2.7540e-04 - dense_27_2_loss: 0.0032 - dense_27_3_loss: 0.0160 - dense_27_4_loss: 1.1956e-04 - dense_27_5_loss: 5.3333e-04 - dense_27_6_loss: 0.0011 - dense_27_7_loss: 1.4624e-04 - dense_27_8_loss: 5.7570e-04 - dense_27_9_loss: 8.3233e-04 - dense_27_accuracy: 1.0000 - dense_27_1_accuracy: 1.0000 - dense_27_2_accuracy: 0.9996 - dense_27_3_accuracy: 0.9980 - dense_27_4_accuracy: 1.0000 - dense_27_5_accuracy: 1.0000 - dense_27_6_accuracy: 1.0000 - dense_27_7_accuracy: 1.0000 - dense_27_8_accuracy: 1.0000 - dense_27_9_accuracy: 1.0000\n",
            "Epoch 10/15\n",
            "313/313 [==============================] - 11s 36ms/step - loss: 0.8409 - dense_27_loss: 0.0386 - dense_27_1_loss: 0.0318 - dense_27_2_loss: 0.1918 - dense_27_3_loss: 0.2465 - dense_27_4_loss: 0.0098 - dense_27_5_loss: 0.0315 - dense_27_6_loss: 0.0990 - dense_27_7_loss: 0.0268 - dense_27_8_loss: 0.0864 - dense_27_9_loss: 0.0787 - dense_27_accuracy: 0.9890 - dense_27_1_accuracy: 0.9881 - dense_27_2_accuracy: 0.9481 - dense_27_3_accuracy: 0.9412 - dense_27_4_accuracy: 0.9981 - dense_27_5_accuracy: 0.9936 - dense_27_6_accuracy: 0.9800 - dense_27_7_accuracy: 0.9956 - dense_27_8_accuracy: 0.9803 - dense_27_9_accuracy: 0.9857\n",
            "Epoch 11/15\n",
            "313/313 [==============================] - 11s 36ms/step - loss: 0.0496 - dense_27_loss: 0.0017 - dense_27_1_loss: 0.0018 - dense_27_2_loss: 0.0087 - dense_27_3_loss: 0.0234 - dense_27_4_loss: 4.9606e-04 - dense_27_5_loss: 0.0026 - dense_27_6_loss: 0.0035 - dense_27_7_loss: 1.6474e-04 - dense_27_8_loss: 0.0051 - dense_27_9_loss: 0.0023 - dense_27_accuracy: 1.0000 - dense_27_1_accuracy: 1.0000 - dense_27_2_accuracy: 0.9996 - dense_27_3_accuracy: 0.9980 - dense_27_4_accuracy: 1.0000 - dense_27_5_accuracy: 0.9997 - dense_27_6_accuracy: 0.9999 - dense_27_7_accuracy: 1.0000 - dense_27_8_accuracy: 0.9990 - dense_27_9_accuracy: 1.0000\n",
            "Epoch 12/15\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.0326 - dense_27_loss: 7.6241e-04 - dense_27_1_loss: 8.9678e-04 - dense_27_2_loss: 0.0055 - dense_27_3_loss: 0.0189 - dense_27_4_loss: 2.1336e-04 - dense_27_5_loss: 0.0011 - dense_27_6_loss: 0.0019 - dense_27_7_loss: 1.0337e-04 - dense_27_8_loss: 0.0020 - dense_27_9_loss: 0.0012 - dense_27_accuracy: 1.0000 - dense_27_1_accuracy: 1.0000 - dense_27_2_accuracy: 0.9996 - dense_27_3_accuracy: 0.9980 - dense_27_4_accuracy: 1.0000 - dense_27_5_accuracy: 1.0000 - dense_27_6_accuracy: 1.0000 - dense_27_7_accuracy: 1.0000 - dense_27_8_accuracy: 0.9996 - dense_27_9_accuracy: 1.0000\n",
            "Epoch 13/15\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.0265 - dense_27_loss: 4.6168e-04 - dense_27_1_loss: 5.4108e-04 - dense_27_2_loss: 0.0046 - dense_27_3_loss: 0.0170 - dense_27_4_loss: 1.3900e-04 - dense_27_5_loss: 6.8075e-04 - dense_27_6_loss: 0.0013 - dense_27_7_loss: 7.4233e-05 - dense_27_8_loss: 8.9135e-04 - dense_27_9_loss: 8.0415e-04 - dense_27_accuracy: 1.0000 - dense_27_1_accuracy: 1.0000 - dense_27_2_accuracy: 0.9996 - dense_27_3_accuracy: 0.9980 - dense_27_4_accuracy: 1.0000 - dense_27_5_accuracy: 1.0000 - dense_27_6_accuracy: 1.0000 - dense_27_7_accuracy: 1.0000 - dense_27_8_accuracy: 0.9999 - dense_27_9_accuracy: 1.0000\n",
            "Epoch 14/15\n",
            "313/313 [==============================] - 11s 36ms/step - loss: 0.0237 - dense_27_loss: 3.1896e-04 - dense_27_1_loss: 3.6099e-04 - dense_27_2_loss: 0.0040 - dense_27_3_loss: 0.0164 - dense_27_4_loss: 9.7696e-05 - dense_27_5_loss: 4.4019e-04 - dense_27_6_loss: 9.1069e-04 - dense_27_7_loss: 5.9673e-05 - dense_27_8_loss: 4.8220e-04 - dense_27_9_loss: 6.0124e-04 - dense_27_accuracy: 1.0000 - dense_27_1_accuracy: 1.0000 - dense_27_2_accuracy: 0.9996 - dense_27_3_accuracy: 0.9980 - dense_27_4_accuracy: 1.0000 - dense_27_5_accuracy: 1.0000 - dense_27_6_accuracy: 1.0000 - dense_27_7_accuracy: 1.0000 - dense_27_8_accuracy: 1.0000 - dense_27_9_accuracy: 1.0000\n",
            "Epoch 15/15\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.0219 - dense_27_loss: 2.4127e-04 - dense_27_1_loss: 2.5427e-04 - dense_27_2_loss: 0.0037 - dense_27_3_loss: 0.0156 - dense_27_4_loss: 8.1767e-05 - dense_27_5_loss: 3.2660e-04 - dense_27_6_loss: 6.7607e-04 - dense_27_7_loss: 4.9351e-05 - dense_27_8_loss: 4.0456e-04 - dense_27_9_loss: 4.4208e-04 - dense_27_accuracy: 1.0000 - dense_27_1_accuracy: 1.0000 - dense_27_2_accuracy: 0.9996 - dense_27_3_accuracy: 0.9980 - dense_27_4_accuracy: 1.0000 - dense_27_5_accuracy: 1.0000 - dense_27_6_accuracy: 1.0000 - dense_27_7_accuracy: 1.0000 - dense_27_8_accuracy: 1.0000 - dense_27_9_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f679968b0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 451
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOc7whHE2BLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "6336182d-8fef-4cee-a62d-bee1980110db"
      },
      "source": [
        "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
        "for example in EXAMPLES:\n",
        "  for_input=np.array([string_to_int(example,30,human_vocab)])\n",
        "  final_for_input=np.array(list(map(lambda x: to_categorical(x,num_classes=len(human_vocab)),for_input)))\n",
        "  s0=np.zeros((1,n_s))\n",
        "  c0=np.zeros((1,n_s))\n",
        "  prediction=model.predict([final_for_input,s0,c0])\n",
        "  prediction = np.argmax(prediction, axis = -1)\n",
        "  output = [inv_machine_vocab[int(i)] for i in prediction]\n",
        "  print(\"input:\", example)\n",
        "  print(\"output:\", ''.join(output),\"\\n\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 452,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input: 3 May 1979\n",
            "output: 1979-05-03 \n",
            "\n",
            "input: 5 April 09\n",
            "output: 2009-04-05 \n",
            "\n",
            "input: 21th of August 2016\n",
            "output: 2016-08-21 \n",
            "\n",
            "input: Tue 10 Jul 2007\n",
            "output: 2007-07-10 \n",
            "\n",
            "input: Saturday May 9 2018\n",
            "output: 2018-05-09 \n",
            "\n",
            "input: March 3 2001\n",
            "output: 2001-03-03 \n",
            "\n",
            "input: March 3rd 2001\n",
            "output: 2001-03-03 \n",
            "\n",
            "input: 1 March 2001\n",
            "output: 2001-03-01 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhrKWK49ImEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example=\"may 05th 79 \"\n",
        "for_input=np.array([string_to_int(example,30,human_vocab)])\n",
        "final_for_input=np.array(list(map(lambda x: to_categorical(x,num_classes=len(human_vocab)),for_input)))\n",
        "s0=np.zeros((1,n_s))\n",
        "c0=np.zeros((1,n_s))\n",
        "prediction=model.predict([final_for_input,s0,c0])\n",
        "prediction = np.argmax(prediction, axis = -1)"
      ],
      "execution_count": 453,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCDtpGXQIqTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention_map(model, input_vocabulary, inv_output_vocabulary, text, n_s = 128, num = 6, Tx = 30, Ty = 10):\n",
        "  attention_map=np.zeros((Ty,Tx))\n",
        "  s0 = np.zeros((1, n_s))\n",
        "  c0 = np.zeros((1, n_s))\n",
        "  layer = model.layers[num]\n",
        "  for_input=np.array([string_to_int(text,Tx,input_vocabulary)])\n",
        "  final_for_input=np.array(list(map(lambda x: to_categorical(x,num_classes=len(input_vocabulary)),for_input)))\n",
        "  prediction=model.predict([final_for_input,s0,c0])\n",
        "  final_prediction=np.argmax(prediction,axis=-1)\n",
        "  outputs=[inv_output_vocabulary[int(i)] for i in final_prediction]\n",
        "  \n",
        "\n",
        "  f = K.function(model.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
        "  r = f([final_for_input, s0, c0])\n",
        "  for t in range(Ty):\n",
        "    for tprime in range(Tx):\n",
        "      attention_map[t][tprime] = r[t][0,tprime,0]\n",
        "  \n",
        "  plt.clf()\n",
        "  fig=plt.figure(figsize=(8,8))\n",
        "  ax=fig.add_subplot(1,1,1)\n",
        "  i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')\n",
        "\n",
        "  ax.set_xticks(list(range(len(text))))\n",
        "  ax.set_xticklabels(text)\n",
        "\n",
        "  ax.set_yticks(list(range(len(outputs))))\n",
        "  ax.set_yticklabels(outputs)\n",
        "  cbaxes = fig.add_axes([0.2, 0, 0.6, 0.03])\n",
        "  cbar = fig.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
        "  cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
        "  ax.grid()\n",
        "\n"
      ],
      "execution_count": 454,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV4mKQEtcDi8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "6bd7214f-8e86-4d42-cf90-d2cc37689bef"
      },
      "source": [
        "plot_attention_map(model,human_vocab,inv_machine_vocab,'may 19, 1999',n_s,7,Tx,Ty)"
      ],
      "execution_count": 455,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9da-bsfgzoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 455,
      "outputs": []
    }
  ]
}